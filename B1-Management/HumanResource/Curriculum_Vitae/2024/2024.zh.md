# NAME

## 联系方式

| Email                                               | TEL | GitHub (375 followers)                                      |
| --------------------------------------------------- | --- | ----------------------------------------------------------- |
| [charl3s.xu@gmail.com](mailto:charl3s.xu@gmail.com) | TEL | [github.com/the0demiurge](https://github.com/the0demiurge/) |

<span style="float:right;">REGION</span>

## 教育经历

| 时间 | 学校       | 学历   | 专业  |
| ---- | ---------- | ------ | ----- |
| TIME | UNIVERSITY | DEGREE | MAJOR |
| TIME | UNIVERSITY | DEGREE | MAJOR |

## 技能

- **数学 / 算法**
  - **深度学习：** 系统性掌握 **强化学习** 并有着丰富实践经验，有 NLP 训练经验与 LLM 应用经验，了解推荐系统；熟练使用 **PyTorch**，熟悉深度学习优化算法原理
  - **机器学习：** 掌握或了解监督（**树模型**、Logistic 回归）、无监督（**聚类**、关联分析）、半监督、对比学习；熟练使用数据分析工具（**pandas**, **numpy**, **matplotlib**）
  - **数据结构与算法**，复杂性理论，A-star 等
- **专业技能**
  - **运筹优化：** MILP (ortools)，启发式优化 (GA/SA/CEM)，凸优化 (cvxpy)
  - **控制论：**线性系统，系统辨识，最优控制
- **工程技能**
  - **编程语言：** **精通Python**，熟悉C/C++、bash、Makefile
  - **Linux：**从2016年至今使用ArchLinux/Ubuntu作为主力系统，管理过7台深度学习服务器，拥有一台VPS
  - **软件工程**(git, CI, Dockerfile, 代码规范性)，**计算机网络**(http, OSI), **数据中间件**(Kafka)

## 工作经历

### COMPANY  -  POSITION <span style="float:right;">TIME</span>

#### 强化学习 + 自动驾驶: 可行性验证

- **S:** Learning-Based 自动驾驶存在因果混淆、推理误差累积等问题，而强化学习不仅能解决这些问题，还可通过探索实现超越人类的驾驶策略。团队在我加入之前已进行一年探索但未能产出成果。
- **A:** 分析问题，主导团队并分工进行了模型结构、仿真环境、训练框架迭代，显著提升训练效率。此外，完成了: ***1.*** 基于 **Transformer** 的模型结构设计与定型；***2.*** 使用信息熵均衡化的**模仿学习**加速RL收敛；***3.*** 应用**有约束强化学习**以提升训练稳定性；***4.*** 使用 **Dagger** 将多个 RL 模型的优点蒸馏到单个模仿学习模型等方法。
- **R:** 实现了从 0 到 1 的突破，成功验证 RL 的可行性与有效性，在 Waymo 公开数据集上达成 **96.1%** 通过率，并通过 dagger 蒸馏方法，将通过率提升到 **98.9%**，为团队争取到了大量计算资源的支持。

#### 强化学习 + 自动驾驶: 停车场业务 **实车落地**

- **S:** 探索强化学习策略落地可能性。RL 落地主要分为两个方向：***1.*** 将 RL 策略蒸馏到大模型；***2.*** 直接接入实车进行控车。这个过程中存在 sim-to-real 的困难。
- **A:** 算法层面探索了多种方案：***1.*** 模型结构变更以适应真实场景；***2.*** 训练环境模拟实车运行机制；***3.*** 通过数据增强提升模型泛化能力和鲁棒性；***4.*** 业务场景专项调优以应对复杂困难交互；***5.*** 初步探索多智能体对交互能力的提升。工程层面完成跨团队合作进行模型落地接口设计、对接与验收。
- **R:** 克服 sim-to-real 困难，初步实现 **完整的实车落地流程**，**初版模型**可成功完成困难 车-车 、 车-闸机 交互，并实现实车 **73%** 无接管通过率（静态入场**100%**成功）；多智能体交互在仿真下实现 **98%** 的安全性。

#### 强化学习 + 自动驾驶: 拟人化数据合成

- **S:** 采集人驾数据很贵且难以采集长尾场景；而通过仿真+自动驾驶算法采集的数据质量较差。强化学习在数据生产方面可同时结合两者优势。
- **A:** ***1.*** 设计了可在不同场景使用的通用 reward；***2.*** 进行实验并证明只需对数复杂度的 RL 模型数可完全覆盖场景池；设计多种拟人化评价标准，结合 Rule-based Reward 与 **GAIL** 实现拟人化驾驶策略训练。
- **R:** 完成可行性论证并使 RL 的能力受到认可，显著提升 RL 驾驶行为的拟人化水平并改善驾驶体感，完成多种拟人化评价指标设计与实现。当前正在进行数据合成流程搭建和进一步试验验证。

#### 总结

- 作为**项目负责人**，抓住主要矛盾并正确把握了团队方向，使工作有效沉淀下了实践经验和技术优势；确定好方向后，开发工作较重时推动形成较好的编码规范，进行项目拆解、时间与分工的规划规划，实验工作较重时组织整体实验规划和 review，通过避免无效工作以提升效率和质量；最后向上争取到了团队的资源和生存空间，使整个团队的工作产出了出色的业务价值并受到认可。

---

### COMPANY  -  POSITION <span style="float:right;">TIME</span>

#### 基于深度强化学习的交通控制算法研究与落地

- **S:** 目前学术界基于 AI 的信号灯控制算法大多基于强假设，没有考虑到感知范围、感知精度、感知延迟等等问题，难以落地。研究使用**强化学习落地**信号灯控制的可能性，在现实条件多种场景超越传统方法。
- **A:** ***1.*** 设计和开发与现实一致的 RL 环境 (state, action, reward)；***2.*** 实现传统控制算法作为 专家经验；***3.*** 实现并优化多种 RL 算法（Rainbow，SAC，PPO），使得 RL 的性能超越传统方法；***4.*** 使用**信息熵**理论解决专家经验中样本分布不平衡问题，使 RL 成功模仿专家经验预训练，显著提升训练效率；***5.*** 设计并实现可泛化的 RL 模型，解决 SOTA 基于 RL 的交通控制模型无法跨场景泛化的问题。
- **R:** 已发表两篇论文: ***1.*** 干线控制：PRIVATE、 ***2.*** 车路协同 + 泛化性：PRIVATE。另有PRIVATE审稿中。

#### 基于最优控制（运筹优化）的车路协同-协作式车速引导

- **S:** 协作式车速引导是自动驾驶 L2 到 L4 车路协同中的一项重要应用。
- **A:** ***1.*** 实现多种车辆能耗模型作为优化目标函数和评价指标；***2.*** 复现 SOTA 车速优化算法，进行纵向车辆轨迹规划；***3.*** 设计基于**最优控制**的控制策略，使用**序列二次规划(SQP)**和**启发式算法**求解，综合考虑多方博弈从而达到整体最优。
- **R:** 单车能耗降低 12%；渗透率 10% 时，自动驾驶车能耗降低 9%。
- 项目已成功落地：PRIVATE。

#### 基于机器学习与运筹优化的大规模智慧交通控制系统

- **S:** 在大数据时代，多交通信号灯的智能控制可以大幅提升交通运行效率，减少拥堵。
- **A:** ***1.*** 使用**聚类算法(K-Means, DBSCAN)**对交通流数据自动**聚类**并打标，作为数据集训练有监督模型(KNN)；***2.*** 使用**有序聚类(谱聚类，Fisher)**对交通数据自动时段划分；***3.*** 使用**混合整数规划(MILP)、启发式算法**对大规模交通控制问题分层次建模求解；***4.*** 设计并实现整体算法工程化架构。
- **R:** 集成到 PRIVATE 产品线中，在PRIVATE实地评测中达到显著优于人类专家手动调整的结果。
- 发表专利PRIVATE、PRIVATE。
- 本项目已落地：PRIVATE

## 学校项目

#### 基于强化学习的智能参数优化算法研究<span style="float:right;">TIME</span>

- **S:** 材料学的反应力场求解是一个很困难的参数优化问题，传统方法常用遗传算法求解，耗时很长。
- **A:** 提出相似材料具有相似优化目标函数假说，设计并实现 RL 环境，包括多种特征与优化模式，实现并改进多种**RL算法(DQN,A3C,DDPG)**和多种传统**最优化算法(共轭梯度法，模拟退火等)**，设计实验并证明了 RL 做参数优化的有效性。
- **R:** ***1.*** 在相似材料的优化性能上取得了与专业反应力场优化软件GarfField相当的水平，大幅超越共轭梯度法、模拟退火算法；***2.*** 取得软件著作权。

## 自我评价

- **身体强壮**，有锻炼习惯，有条件时常组织远足和骑行。
- **组织能力：** 带领团队在强化学习自动驾驶项目取得了出色的成果。
- **学习能力：** 自学网站建设、材料学、交通工程理论、自动驾驶等，快速上手并逐渐深入掌握多种领域
- **善于总结**，在线笔记: [notes.love-y.eu](https://notes.love-y.eu)。
- **Geek 精神**，使用编程改善生活：[CharlesScripts](https://github.com/the0demiurge/CharlesScripts), 本项目获得 **1k Star** 与 **722 Fork**; 包括本简历也是使用[这个工具](https://notes.love-y.eu/B1-Management/HumanResource/Curriculum_Vitae/)生成。
- **个人博客:** [the0demiurge.blogspot.com](https://the0demiurge.blogspot.com/)
- **Google Scholar:** PRIVATE
- **热爱游戏**: 博览群游，曾写博客[《 关于电子游戏的随想 》](https://the0demiurge.blogspot.com/2020/08/blog-post.html)
