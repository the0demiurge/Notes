1. 对于real-world rl问题，通常都区分了训练和评估两个过程，与bandit不同。bandit问题不区分训练和评估过程，要求exploration和exploitation找平衡，而rl问题没有必要寻找这种平衡，而是充分利用好这两个过程即可。
2. 当前常见的RL采样过程都很有问题，采样策略不好，会逐渐收敛到原本搜索到的局优策略附近，导致对全局的把握越来越差（数据样本均衡度变差）。[Experience Selection in Deep Reinforcement Learning for Control p15](https://jmlr.csail.mit.edu/papers/volume19/17-131/17-131.pdf)
3. RL的最终目的是通过与环境的交互学习，使得最终学到一个最优策略，无需在训练过程中最大化期望回报，训练过程中我们应当鼓励探索，力求对环境准确建模。最简单的方法就是对环境均匀采样，然而我们可能会存在多种采样偏好，具体哪种采样偏好更有有待讨论：
   1. 偏好采样高价值状态，因为RL的最终目标是最大化期望回报，尤其在正确动作极少的情况
   2. 偏好采样低价值状态，用于回避最坏的情况
   3. 偏好采样未采样过的状态，对于确定性时不变环境比较容易有整体把握，但对于时变环境可能会产生较大偏差（因为环境变了但没跟上环境的变化，尤其是重要的地方的变化，比如高和低价值状态的变化）
   4. 偏好采样难以预测的状态，即便都是未采样过的状态，有的函数曲线非常平滑，很容易预测，那么机器学习模型很容易就能泛化到，就不必浪费重要的采样资源，对未知的事物有好奇心（ICM），对已知的事物好奇心降低（对于随机或时变系统还时需要保持一定好奇心）。问题在于当遇到随机过程后就会被吸引到注意力，浪费资源，因此还可能需要对随机性进行评估，这个随机性可能还需要通过因果推理获取，直接ICM存在一些问题
4. 强化学习并非万能，有的问题靠随机探索很难遇到正确答案，因此强化学习的未来需要考虑归纳和演绎推理，不能总是瞎猜，需要用推理来赋能探索和规划才能接近人类的学习能力，[因果学习&RL](https://zhuanlan.zhihu.com/p/372917922)
5. 对于多维的动作（其实本质上就是multi agent），只有一个reward时，会遇到credit assignment的问题，在优化领域是维度灾难
