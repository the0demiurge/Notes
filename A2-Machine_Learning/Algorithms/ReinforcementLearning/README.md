# Introduction

强化学习与其他机器学习方法非常不同。有监督学习类似于数据库检索和近邻匹配，需要外在的帮助（打标签）；无监督学习本质上只是对数据分布做一些估计和处理；而强化学习最核心的目标是通过尝试与环境做交互来学到一个策略，不一定需要人工手把手教agent做事情（有监督学习），即便手把手教（抛砖）过也能通过探索找到更好的问题解决方案（引玉）。

## Environment Model

综上所述，强化学习的模型就包括两部分，一个是环境，另一个是用于和环境交互的agent。不同的环境存在着不同特性，对于环境的不同特性，不同学习方法的学习效率也会有很大不同：

1. 确定性，就好比牛顿力学三定律描述下的简单系统，输入和内在状态确定那么输出就确定，这就意味着一方面来说，不必在相同状况下与环境做出相同的交互，因为结果必定相同；另一方面知识永不过时，不必更新。
2. 不平衡性，靠随机探索只会有极低概率成功，比如下棋、starcraft、解魔方、机械臂抓取物品等，这种问题如果不依赖专家经验的指导或逻辑推理，纯靠随机探索怎么做都是错，肯定还是什么都学不会
3. 随机性，比如赌博或购买彩票，对于相同的状态采取相同动作结果会有所不同（掷骰子），需要在相同状态下多次作出相同交互，来估计不同动作的好坏。
4. 时变性，好比股票交易市场，股票涨跌是随机的，影响股票价格的规则也是随时间在发生改变的。这个过程中有几种方案：
   1. 通过大量与环境的交互跟随上环境的变化（活到老学到老，但是很难）
   2. 在改变中发现其中的不变性（透过现象看到本质，万变不离其宗，更难）
   3. 微元法，如果在短时间内环境不容易出现剧烈变化，就可以把短时间内的环境当作时不变环境处理（当然这就需要不断更新agent了，很麻烦）
5. 部分可观测性，造成结果的原因只能被观测到一小部分，环境的内部状态就变成了未知，对于即便是确定的环境，在agent看来也不再确定。

agent是通过与环境交互学习的，那么最常见的环境模型包含三个接口：

- 观测，agent从环境中观测到信息
- 动作，agent基于观测到的信息对环境作出反应
- 回报，环境对agent的动作给出评价

在控制理论中，环境模型常用这种方式描述：
$$
\array{
    x(k+1) &= f_A(x(k))+f_B(u(k))\\
    y(k) &= f_C(x(k))+f_D(u(k))
}
$$
其中：

- x(k)表示k时刻的环境内部状态，RL中常用符号为s(t)
- y(k)表示k时刻的观测信息，RL中常用符号为o(t)；可以用同样的方法算出回报信息，RL中常用符号为r(t)
- u(k)表示k时刻agent的动作，RL中常用符号为a(t)
- 四个f表示四种函数，它们可能是时变的，随机的，多输入多输出的

